{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 5. 3. 6. 1. 1. 2. 1. 3. 3. 1. 1. 3. 2. 3. 4. 4. 3. 1. 5. 1. 1. 1. 3.\n",
      " 1. 3. 2. 5. 1. 1. 1. 3. 3. 5. 3. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1.]\n",
      "dict_keys(['_id', 'id', 'title', 'content', 'platform', 'tags', 'date', '__v', 'topic'])\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from string import punctuation\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "# !!! MAKE SURE TO USE SVC.decision_function(X), NOT SVC.predict(X) !!!\n",
    "# (this makes ``continuous-valued'' predictions)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    \n",
    "    return input_string.lower().split()\n",
    "\n",
    "def extract_dictionary(article_list):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = {}\n",
    "\n",
    "    index = 0\n",
    "    for article in article_list: # for each tweet \n",
    "        for word in extract_words(article): # for each word in the tweet \n",
    "            if word not in word_list and (len(word)>2):\n",
    "                word_list[word] = index # assign the index \n",
    "                index += 1\n",
    "    pass\n",
    "\n",
    "    return word_list\n",
    "\n",
    "def extract_feature_vectors(article_list, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "    \n",
    "    num_articles = len(article_list)\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_articles, num_words))\n",
    "    \n",
    "# populate feature matrix \n",
    "    tweet_num = 0\n",
    "    for tweet in article_list: # for each tweet \n",
    "        for word in extract_words(tweet): # for each word in the tweet \n",
    "            if (len(word)>2):\n",
    "                feature_matrix[tweet_num][word_list[word]] = 1 #  if it's in there set it to 1\n",
    "        tweet_num += 1\n",
    "    pass\n",
    "        \n",
    "    return feature_matrix\n",
    "\n",
    "### END FUNCTIONS ###\n",
    "with open(\"fifty_articles.json\", encoding=\"utf-8\") as read_file1:\n",
    "    data = json.load(read_file1)\n",
    "# this file has ALL the blog posts \n",
    "with open(\"blogposts (1).json\", encoding=\"utf-8\") as read_file2:\n",
    "    data_all = json.load(read_file2)\n",
    "    \n",
    "## extract labels from text file \n",
    "y_train = read_vector_file('article_training_labels_50.txt')\n",
    "\n",
    "## extract test article list \n",
    "articles = [] \n",
    "for article in range(len(data['response']['posts'])):\n",
    "    articles.append(data['response']['posts'][article]['body'])\n",
    "\n",
    "## extract full article list \n",
    "articles_all = [] \n",
    "articles_all_success_indices=[] \n",
    "for article in range(len(data_all)):\n",
    "    articles_all.append(data_all[article]['content'])\n",
    "\n",
    "# extract titles from json file \n",
    "titles = [] \n",
    "for title in range(len(data['response']['posts'])):\n",
    "    titles.append(data['response']['posts'][title]['title'])\n",
    "\n",
    "# extract titles from json file with all the posts\n",
    "titles_all = [] \n",
    "for title in range(len(data_all)):\n",
    "    titles_all.append(data_all[title]['title'])\n",
    "   \n",
    "dictionary = extract_dictionary(articles) \n",
    "dictionary_all = extract_dictionary(articles_all)\n",
    "dictionary_all_titles = extract_dictionary(titles_all)\n",
    "dictionary_all = dict(dictionary_all, **dictionary_all_titles)\n",
    "\n",
    "\n",
    "### Show Review, Album Review, Concert Review, Interview, UCLA Radio News, UCLA Radio Sports, UCLA Radio Comedy, Film Review, \n",
    "\n",
    "\n",
    "\n",
    "X_train = extract_feature_vectors(articles, dictionary_all)\n",
    "X_test = extract_feature_vectors(articles_all, dictionary_all)\n",
    "\n",
    "y_test=OneVsRestClassifier(LinearSVC(random_state=0)).fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#for i in range(len(y_test)):\n",
    "#    if y_test[i] == 3:\n",
    "#        print(titles_all[i])\n",
    "\n",
    "\n",
    "# I want to add a new category for data_all[article]['content'] but replace content with 'topic' \n",
    "category_list = [\"Invalid Tag\",\"Show Review\", \"Music Review\", \"Interview\", \"Sports\", \"News\",\"Entertainment\"] \n",
    "\n",
    "\n",
    "for article in range(len(data_all)):\n",
    "    my_topic = category_list[int(y_test[article])]\n",
    "    data_all[article]['topic']=my_topic\n",
    "    \n",
    "print(y_train)\n",
    "print(data_all[4].keys())\n",
    "#with open('dump2.json', 'w') as f:  # writing JSON object\n",
    "#    json.dump(data_all, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
